#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

## Namespace to deploy pulsar
namespace: pulsar
namespaceCreate: true

## Pulsar Metadata Prefix
##
## By default, pulsar stores all the metadata at root path.
## You can configure to have a prefix (e.g. "/my-pulsar-cluster").
## If you do so, all the pulsar meta
metadataPrefix: ""

## If persistence is enabled, components that have state will
## be deployed with PersistentVolumeClaims, otherwise, for test
## purposes, they will be deployed with emptyDir
persistence: true

components:
  # zookeeper
  zookeeper: true
  # bookkeeper
  bookkeeper: true
  # bookkeeper - autorecovery
  autorecovery: true
  # broker
  broker: true
  # proxy
  proxy: true
  # toolset
  toolset: true
  # pulsar manager
  pulsar_manager: true

monitoring:
  # monitoring - prometheus
  prometheus: true
  # monitoring - grafana
  grafana: true
  # monitoring - node_exporter
  node_exporter: true

## External DNS
## templates/external-dns.yaml
## templates/external-dns-rbac.yaml
external_dns:
  enabled: false
  component: external-dns
  policy: upsert-only
  registry: txt
  owner_id: pulsar
  domain_filter: pulsar.example.local
  provider: google
  providers:
    google:
      # project: external-dns-test
      project:

## Components Stack: control center ingress
## templates/control-center-ingress.yaml
ingress:
  controller:
    enabled: false
    rbac: true
    component: nginx-ingress-controller
    replicaCount: 1
    # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
    tolarations: []
    gracePeriod: 300
    annotations: {}
    ports:
      http: 80
      https: 443
  control_center:
    enabled: true
    component: control-center
    tls:
      enabled: false
    annotations: {}

# TLS
# templates/tls-certs.yaml
tls:
  enabled: false
  # common settings for generating certs
  common:
    # 90d
    duration: 2160h
    # 15d
    renewBefore: 360h
    organization:
      - pulsar
    keySize: 4096
    keyAlgorithm: rsa
    keyEncoding: pkcs8
  # settings for generating certs for proxy
  proxy:
    enabled: false
    cert_name: tls-proxy
    commonName: example.com
    dnsNames:
    - "*"
    uriSANs:
    - "*"
    ipAddresses:
    - "*"
  # settings for generating certs for broker
  broker:
    enabled: false
    cert_name: tls-broker
    commonName: example.com
    dnsNames:
    - "*"
    uriSANs:
    - "*"
    ipAddresses:
    - "*"

## cert-manager
# templates/tls-cert-issuer.yaml
certs:
  internal_issuer:
    enabled: false
    component: internal-cert-issuer
    type: selfsigning
  public_issuer:
    enabled: false
    component: public-cert-issuer
    type: acme
  issuers:
    selfsigning:
    acme:
      # You must replace this email address with your own.
      # Let's Encrypt will use this to contact you about expiring
      # certificates, and issues related to your account.
      email: contact@example.local
      # change this to production endpoint once you successfully test it
      # server: https://acme-v02.api.letsencrypt.org/directory
      server: https://acme-staging-v02.api.letsencrypt.org/directory
      solvers:
        clouddns:
          # TODO: add a link about how to configure this section
          project: "[YOUR GCP PROJECT ID]"
          serviceAccountSecretRef:
            name: "[NAME OF SECRET]"
            namespace: "[NAMESPACE OF THE SECRET]"
            key: "[KEY OF SECRET]"
  lets_encrypt:
    # TODO: add a link about how to configure lets encrypt CA
    ca_ref:
      secretName: [SECRET STORES lets encrypt CA]
      keyName: [KEY IN THE SECRET STORES let encrypt CA]

## Which pulsar image to use
images:
  pulsar:
    # repository: streamnative/pulsar
    # tag: 2.4.0-d1d5aba08
    repository: apachepulsar/pulsar-all
    tag: 2.4.2
    pullPolicy: IfNotPresent
  prometheus:
    repository: prom/prometheus
    tag: v1.6.3
    pullPolicy: IfNotPresent
  grafana:
    repository: streamnative/apache-pulsar-grafana-dashboard-k8s
    tag: 0.0.4
    pullPolicy: IfNotPresent
  pulsar_manager:
    repository: apachepulsar/pulsar-manager
    tag: v0.1.0
    pullPolicy: IfNotPresent
  node_exporter:
    repository: prom/node-exporter
    tag: v0.16.0
    pullPolicy: "IfNotPresent"
  nginx_ingress_controller:
    repository: quay.io/kubernetes-ingress-controller/nginx-ingress-controller
    tag: 0.26.2
    pullPolicy: "IfNotPresent"

## Admin
## templates/pulsar-admin-secret.yaml
##
## This admin credentials will be used for configuring both grafana and
## pulsar-manager
admin:
  user: pulsar
  password: happypulsaring

## Pulsar: Zookeeper cluster
## templates/zookeeper-statefulset.yaml
##
zookeeper:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: zookeeper
  # the number of zookeeper servers to run. it should be an odd number larger than or equal to 3.
  replicaCount: 3
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
  tolarations: []
  gracePeriod: 0
  resources:
    requests:
      memory: 256Mi
      cpu: 0.1
  volumes:
    # use a persistent volume or emptyDir
    persistence: true
    data:
      name: data
      size: 50Gi
      local_storage: false
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
        # type: pd-ssd
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
    dataLog:
      name: datalog
      size: 10Gi
      local_storage: false
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
        # type: pd-ssd
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
  ## Zookeeper configmap
  ## templates/zookeeper-configmap.yaml
  ##
  configData:
    PULSAR_MEM: >
      "
      -Xms64m -Xmx128m
      -Dcom.sun.management.jmxremote
      -Djute.maxbuffer=10485760
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+AggressiveOpts
      -XX:+DoEscapeAnalysis
      -XX:+DisableExplicitGC
      -XX:+PerfDisableSharedMem
      -Dzookeeper.forceSync=no
      "
    PULSAR_GC: >
      "
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      "
  ## Zookeeper service
  ## templates/zookeeper-service.yaml
  ##
  service:
    annotations:
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  ## Zookeeper PodDisruptionBudget
  ## templates/zookeeper-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1

## Pulsar: Bookkeeper cluster
## templates/bookkeeper-statefulset.yaml
##
bookkeeper:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: bookie
  ## BookKeeper Cluster Initialize
  ## templates/bookkeeper-cluster-initialize.yaml
  metadata:
    ## Set the resources used for running `bin/bookkeeper shell initnewcluster`
    ##
    resources:
      # requests:
        # memory: 4Gi
        # cpu: 2
  replicaCount: 4
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
  tolarations: []
  gracePeriod: 0
  resources:
    requests:
      memory: 512Mi
      cpu: 0.2
  volumes:
    # use a persistent volume or emptyDir
    persistence: true
    journal:
      name: journal
      size: 10Gi
      local_storage: false
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
        # type: pd-ssd
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
    ledgers:
      name: ledgers
      size: 50Gi
      local_storage: false
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
        # type: pd-ssd
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
  ## Bookkeeper configmap
  ## templates/bookkeeper-configmap.yaml
  ##
  configData:
    # `BOOKIE_MEM` is used for `bookie shell`
    BOOKIE_MEM: >
      "
      -Xmx128m
      -XX:MaxDirectMemorySize=128m
      "
    # we use `bin/pulsar` for starting bookie daemons
    PULSAR_MEM: >
      "
      -Xms128m
      -Xmx256m
      -XX:MaxDirectMemorySize=256m
      -Dio.netty.leakDetectionLevel=disabled
      -Dio.netty.recycler.linkCapacity=1024
      -XX:+UseG1GC -XX:MaxGCPauseMillis=10
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+AggressiveOpts
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
      -XX:+PrintGCDetails
      -XX:+PrintGCTimeStamps
      -XX:+PrintGCApplicationStoppedTime
      -XX:+PrintHeapAtGC
      -verbosegc
      -XX:G1LogLevel=finest
      "
    # configure the memory settings based on jvm memory settings
    dbStorage_writeCacheMaxSizeMb: "32"
    dbStorage_readAheadCacheMaxSizeMb: "32"
    dbStorage_rocksDB_writeBufferSizeMB: "8"
    dbStorage_rocksDB_blockCacheSize: "8388608"
  ## Bookkeeper Service
  ## templates/bookkeeper-service.yaml
  ##
  service:
    annotations:
      publishNotReadyAddresses: "true"
  ## Bookkeeper PodDisruptionBudget
  ## templates/bookkeeper-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1

## Pulsar: Bookkeeper AutoRecovery
## templates/autorecovery-statefulset.yaml
##
autorecovery:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: recovery
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
  tolarations: []
  gracePeriod: 0
  resources:
    requests:
      memory: 64Mi
      cpu: 0.05
  ## Bookkeeper auto-recovery configmap
  ## templates/autorecovery-configmap.yaml
  ##
  configData:
    BOOKIE_MEM: >
      "
      -Xms64m -Xmx64m
      "

## Pulsar Zookeeper metadata. The metadata will be deployed as
## soon as the last zookeeper node is reachable. The deployment
## of other components that depends on zookeeper, such as the
## bookkeeper nodes, broker nodes, etc will only start to be
## deployed when the zookeeper cluster is ready and with the
## metadata deployed
pulsar_metadata:
  component: pulsar-init
  ## set an existing configuration store
  # configurationStore:

## Pulsar: Broker cluster
## templates/broker-statefulset.yaml
##
broker:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: broker
  replicaCount: 3
  ports:
    http: 8080
    https: 8443
    pulsar: 6650
    pulsarssl: 6651
  # nodeSelector:
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
  tolarations: []
  gracePeriod: 0
  resources:
    requests:
      memory: 512Mi
      cpu: 0.2
  ## Broker configmap
  ## templates/broker-configmap.yaml
  ##
  configData:
    PULSAR_MEM: >
      "
      -Xms128m -Xmx256m -XX:MaxDirectMemorySize=256m
      -Dio.netty.leakDetectionLevel=disabled
      -Dio.netty.recycler.linkCapacity=1024
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+AggressiveOpts
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
      "
    PULSAR_GC: >
      "
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      "
    managedLedgerDefaultEnsembleSize: "3"
    managedLedgerDefaultWriteQuorum: "3"
    managedLedgerDefaultAckQuorum: "2"

  # Enable or disable broker authentication and authorization.
  auth:
    authentication:
      enable: false
      # Enable JWT authentication
      # If the token is generated by a secret key, set the usingSecretKey as true.
      # If the token is generated by a private key, set the usingSecretKey as false.
      usingSecretKey: "true"
    authorization:
      enable: false

  ## Broker service
  ## templates/broker-service.yaml
  ##
  service:
    annotations: {}
  ## Broker PodDisruptionBudget
  ## templates/broker-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1

## Pulsar: Proxy Cluster
## templates/proxy-statefulset.yaml
##
proxy:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: proxy
  replicaCount: 3
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolarations: []
  gracePeriod: 0
  resources:
    requests:
      memory: 128Mi
      cpu: 0.2
  ## Proxy configmap
  ## templates/proxy-configmap.yaml
  ##
  configData:
    PULSAR_MEM: >
      "
      -Xms64m -Xmx64m -XX:MaxDirectMemorySize=64m
      -Dio.netty.leakDetectionLevel=disabled
      -Dio.netty.recycler.linkCapacity=1024
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+AggressiveOpts
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
      "
    PULSAR_GC: >
      "
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      "
  ## Proxy service
  ## templates/proxy-service.yaml
  ##
  ports:
    http: 80
    https: 443
    pulsar: 6650
    pulsarssl: 6651
  service:
    annotations: {}
    type: LoadBalancer
  ## Proxy PodDisruptionBudget
  ## templates/proxy-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1

## Pulsar ToolSet
## templates/toolset-deployment.yaml
##
toolset:
  component: toolset
  useProxy: true
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolarations: []
  gracePeriod: 0
  resources:
    requests:
      memory: 256Mi
      cpu: 0.1
  ## Bastion configmap
  ## templates/bastion-configmap.yaml
  ##
  configData:
    PULSAR_MEM: >
      "
      -Xms64M
      -Xmx128M
      -XX:MaxDirectMemorySize=128M
      "

#############################################################
### Monitoring Stack : Prometheus / Grafana
#############################################################

## Monitoring Stack: Prometheus
## templates/prometheus-deployment.yaml
##
prometheus:
  component: prometheus
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolarations: []
  gracePeriod: 0
  resources:
    requests:
      memory: 256Mi
      cpu: 0.1
  volumes:
    # use a persistent volume or emptyDir
    persistence: true
    data:
      name: data
      size: 10Gi
      local_storage: false
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
        # type: pd-standard
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
  ## Prometheus service
  ## templates/prometheus-service.yaml
  ##
  service:
    annotations: {}
    ports:
    - name: server
      port: 9090

## Monitoring Stack: Grafana
## templates/grafana-deployment.yaml
##
grafana:
  component: grafana
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolarations: []
  gracePeriod: 0
  port: 3000
  resources:
    requests:
      memory: 250Mi
      cpu: 0.1
  ## Grafana service
  ## templates/grafana-service.yaml
  ##
  service:
    type: ClusterIP
    annotations: {}
    ports:
    - name: server
      port: 3000


## Monitoring Stack: node_exporteer
## templates/node-exporter.yaml
##

node_exporter:
  component: node-exporter
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9100"
  limits:
    cpu: 10m
    memory: 50Mi
  requests:
    cpu: 10m
    memory: 50Mi

## Components Stack: pulsar_manager
## templates/pulsar-manager.yaml
##

pulsar_manager:
  component: pulsar-manager
  port: 9527
  replicaCount: 1
  # nodeSelector:
  # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolarations: []
  gracePeriod: 0
  resources:
    requests:
      memory: 250Mi
      cpu: 0.1
  ## Pulsar manager service
  ## templates/pulsar-manager-service.yaml
  ##
  service:
    type: ClusterIP
    annotations: {}
    ports:
      - name: server
        port: 9527
  redirect_host: 127.0.0.1

## Components Stack: pulsar operators rbac
## templates/pulsar-operators-rbac.yaml
##

rbac:
  enable: true
  roleName: pulsar-operator
  roleBindingName: pulsar-operator-cluster-role-binding
